{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d714a9",
   "metadata": {},
   "source": [
    "# Creating dataset in json format\n",
    "\n",
    "dataset will be in the json format {en : \"text\", ne : \"translated-text\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dde618f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NNC', 'bible', 'NLC', 'PR_improved', 'the guardian', 'gnome_final', 'globalvoices_improved'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_file_names_without_extension(folder_path):\n",
    "    # Get a list of all files and directories in the folder\n",
    "    files_and_dirs = os.listdir(folder_path)\n",
    "\n",
    "    # Create an empty set to store the file names without extensions\n",
    "    file_names_set = set()\n",
    "\n",
    "    # Filter out only the files and add their base names to the set\n",
    "    for file_name in files_and_dirs:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            file_names_set.add(base_name)\n",
    "\n",
    "    return file_names_set\n",
    "\n",
    "\n",
    "folder_path = \"./dataset\"  # Replace this with the actual folder path\n",
    "file_names_set = get_file_names_without_extension(folder_path)\n",
    "\n",
    "# Print the set of file names without extensions\n",
    "print(file_names_set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2036be6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NLC',\n",
       " 'NNC',\n",
       " 'PR_improved',\n",
       " 'bible',\n",
       " 'globalvoices_improved',\n",
       " 'gnome_final',\n",
       " 'the guardian'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "750d08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read().strip().split('\\n')\n",
    "\n",
    "def combine_data(english_data, nepali_data, combined_data = []):\n",
    "    \n",
    "    for eng, nep in zip(english_data, nepali_data):\n",
    "        data_pair = {\"en\": eng, \"ne\": nep}\n",
    "        combined_data.append(data_pair)\n",
    "    return combined_data\n",
    "\n",
    "def write_combined_data(combined_data, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(combined_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    # Replace 'english.txt' and 'nepali.txt' with your file paths\n",
    "#     english_data = read_file('./dataset/globalvoices_improved.en')\n",
    "#     nepali_data = read_file('./dataset/globalvoices_improved.ne')\n",
    "\n",
    "    for file_name in file_names_set:\n",
    "        english_data = read_file(\"./dataset/\" + file_name + \".en\")\n",
    "        nepali_data = read_file(\"./dataset/\" + file_name + \".ne\")\n",
    "        combined_data = combine_data(english_data, nepali_data)\n",
    "\n",
    "#     combined_data = combine_data(english_data, nepali_data)\n",
    "\n",
    "    # Replace 'output.json' with the desired output file name\n",
    "    write_combined_data(combined_data, 'dataset.json')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "893eb270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "319d0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f28c79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json_file('dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41b0b98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177334"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d53037fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3731.59it/s]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 170.11it/s]\n",
      "Generating train split: 177334 examples [00:00, 185018.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "899754d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ne'],\n",
       "        num_rows: 177334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e23e5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens .\n",
      "        \n",
      "Token: Traceback (most recent call last):\n",
      "  File \"/Users/cohlem/anaconda3/bin/huggingface-cli\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/cohlem/anaconda3/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 45, in main\n",
      "    service.run()\n",
      "  File \"/Users/cohlem/anaconda3/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 148, in run\n",
      "    token = getpass(\"Token: \")\n",
      "  File \"/Users/cohlem/anaconda3/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/Users/cohlem/anaconda3/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6c33b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing dataset shards to the dataset hub:   0%|                                                                     | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|                                                                          | 0/178 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  24%|███████████████▍                                                | 43/178 [00:00<00:00, 426.90ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|███████████████████████████████████████████████████████████████| 178/178 [00:00<00:00, 604.36ba/s]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:26<00:00, 26.06s/it]\n",
      "Downloading metadata: 100%|█████████████████████████████████████████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 73.7kB/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub(\"CohleM/english-to-nepali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab69cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnings",
   "language": "python",
   "name": "learnings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
