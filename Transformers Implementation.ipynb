{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16370bf",
   "metadata": {},
   "source": [
    "## MultiHead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff05993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70293bd",
   "metadata": {},
   "source": [
    "## Input Embeddings\n",
    "\n",
    "Since, models don't have notion of words, We convert every word in a sequence into a vector representation of specific dimension (256 or 512 ...). There are certain processes involved in doing so. \n",
    "\n",
    "First, we should have a vocabulary, For example: think of it as a dictionary which contains letter a,b,c,d...z and the letters have their own indexes such as a = 0, b = 1, ... z = 25. so when we have a new letter presented to us, this embedding layer will be applied to that letter. for example c,d will be mapped to 3,4. \n",
    "\n",
    "So instead of having letter in our vocabulary we have words. So, with the help of this vocabulary we map our words in the sequence to the index in its original vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8c8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## embeddings \n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a batch of sequence of words, batch_size, sequence_length -> batch_size, sequence_length, d_model\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0672de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7737908c",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "In Layer Normalization, normalization is done across all the features $x_{i,k}$  than across all the batches, this prcoess removes the dependency input sequences with each other.\n",
    "\n",
    "First, We calculate mean and standard deviation. \n",
    "\n",
    "\\begin{gather} \\mu_i = \\frac{1}{K} \\sum_{k=1}^{K} x_{i,k} \\\\ \\sigma_i^2 = \\frac{1}{K} \\sum_{k=1}^{K} (x_{i,k} - \\mu_i)^2 \\\\ \\end{gather}\n",
    "\n",
    "\n",
    "Then we normalize each sample such that the elements in the sample have zero mean and unit variance. \n",
    "Ïµ\n",
    " is for numerical stability in case the denominator becomes zero by chance.\n",
    " \n",
    " $$\\hat{x}_{i,k} = \\frac{x_{i,k}-\\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$$\n",
    " \n",
    " Finally, there is a scaling and shifting step. \n",
    "Î³\n",
    " and \n",
    "Î²\n",
    " are learnable parameters.\n",
    " \n",
    " $$y_i = \\gamma \\hat{x}_{i} + \\beta \\equiv {\\text{LN}}_{\\gamma, \\beta} (x_i)$$\n",
    " \n",
    "These parameters $\\gamma$ and $\\beta$ introduce fluctuations in the normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5dbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 10**-6\n",
    "        \n",
    "        ## specifying nn.Parameter will add requires_grad_ to that parameter so it will be a learnable parameter.\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.ones(1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # (batch, seq_len, 1)\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        # (batch, seq_len, 1)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "        \n",
    "        # dimension is batch,seq_len, d_model\n",
    "        x = ( self.gamma * (x - mean) / (std + self.eps) ) + self.beta\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dfac1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LN = LayerNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17d040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LN(torch.rand(2,3,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b727f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eee4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = InputEmbedding(2000,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8da12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3820a25c",
   "metadata": {},
   "source": [
    "![(https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png)](https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e4cb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eac1dd75",
   "metadata": {},
   "source": [
    "## MultiHeadAttention\n",
    "\n",
    "Here comes the most important mechanism that powers whole the LLM industry, **attention** \n",
    "\n",
    "Attention in general refers to focus on some specific section of words while ignoring others. Attention is used to understand the context of a word in a sequence.\n",
    "\n",
    "The concept of self-attention is to utilize the entire sequence to compute a weighted average of each token's embedding instead of relying on a fixed embedding for each token like word2vec. Embeddings that are generated this way are called contextualized embeddings. This can be restated as self-attention generating a new sequence of embeddings $x_1', \\ldots, x_n'$ when given a sequence of token embeddings $x_1, \\ldots, x_n$, where each new embedding $x_i'$ is a linear combination of all the $x_j$ in the sequence.\n",
    "\n",
    "$x_i' = \\sum_{j=1}^{n} w_{ji} x_j$\n",
    "\n",
    "The coefficients $w_{ji}$ are called attention weights and are normalized so that:\n",
    "\n",
    "$\\sum_{j=1}^{n} w_{ji} = 1$\n",
    "\n",
    "So, the magic of paying attention is enabled by these attention weights.\n",
    "\n",
    "For example, letâ€™s consider these two sentences.\n",
    "\n",
    "    I love cool, crisp fall weather.\n",
    "    Donâ€™t fall on your way to the gym.\n",
    "\n",
    "  The word fall in the first sentence denotes weather by looking at words like cool and crips whereas fall in the second sentence denotes actually falling by looking at words like way and gym.\n",
    "\n",
    "\n",
    "Let's discuss how we construct attention weights and the final embedding representation.\n",
    "\n",
    "### Scaled dot-product attention\n",
    "\n",
    "Scaled Dot-Product Attention is used to calculate the attention weights.\n",
    "The first step in calculating self-attention is to project three vectors from each of the encoderâ€™s input vectors (in this case, token embeddings). So for each word, we project three matrices,  ð‘„,ð¾,ð‘‰  and which are called Query vector, a Key vector, and a Value vector and each has a dimension of  ð‘‘ð‘˜ . \n",
    "\n",
    "These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\n",
    "The dot product acts as a similarity function which determines how much the query and key vectors relate to each other. If queries and keys are similar, they will result in a significant dot product.\n",
    "\n",
    "The output is calculated as a weighted sum of the values, with each value's weight determined by the query's compatibility function with its corresponding key.\n",
    "\n",
    "\n",
    "\n",
    "To obtain the final weights on the values, first, the dot product of the query with all keys is computed and then normalized by $\\sqrt{d_{k}}$. Then a softmax function is applied. Finally, $V$ is multiplied with the previous output.\n",
    "\n",
    "The final output is:\n",
    "\n",
    "$Attention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "The intuition is that the softmax reweights between 0 and 1 (kind of like probability for each word), which sums upto 1 for all the words. So, multiplying this probability with $V$ determines the contribution of each word against each other. \n",
    "\n",
    "\n",
    "\n",
    "If you don't understand the how we calculate the attention weights and the final embedding representation then, let's understand this with the help of an analogy.\n",
    "\n",
    "Suppose you want to make something to eat for dinner. But you don't know how to. But you've got a Recepie book that tells you what ingredients to use let the Recepie book be our QUERY, now you go to a supermarket to buy these recepies, the ingredients the supermarket has in their shelves is the KEY, and now you look at your recepie book and ingredients in the shelves to find how similar they are, which becomes our attention weights. Now that you've found how similar these ingredients are, you update your shopping cart based on the similarity, which is, multiplying the attention weights with VALUES. That's it Now you have your ingredient, which is the embedding representation which will be used for Language modeling later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18f60f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"This class resembles to the sequence of the above multiheadattenion picture\"\"\"\n",
    "    ##self, input_sequence, head_size, embedding_dimention\n",
    "    def __init__(self, d_model: int, h: int, dropout: float ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.h = h\n",
    "        \n",
    "        assert d_model % h == 0, \"d_model is not divisible by head\"\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_k = d_model // h\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias = False)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(query, key, value, mask, dropout: nn.Dropout ):\n",
    "        d_k = query.shape[-1]\n",
    "        #dot product between Q and K\n",
    "        attention_weights = query @ key.transpose(-2,-1)\n",
    "        \n",
    "        #scaling\n",
    "        attention_weights = attention_weights / math.sqrt(d_k)\n",
    "        \n",
    "        #masking\n",
    "        if mask is not None:\n",
    "            attention_weights = attention_weights.masked_fill_(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = attention_weights.softmax(dim = -1)\n",
    "        \n",
    "        #dropout\n",
    "        if dropout is not None:\n",
    "            attention_weights = dropout(attention_weights)\n",
    "            \n",
    "        return attention_weights @ key, attention_weights\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        \n",
    "        #q,k,v are embeddings of whole batch of sequence, so their size would be batch_size, sequence_length, d_model (embedding dimension)\n",
    "        query = self.W_Q(q)\n",
    "        key = self.W_K(k)\n",
    "        value = self.W_V(v)\n",
    "        \n",
    "        #divide the q,k,v into different h heads\n",
    "        \n",
    "        #query initially had size of (batch_size, sequence_length, d_model)\n",
    "        #and we split the d_model which is the embedding into different heads with each size of d_k = d_model/h\n",
    "        #We finally call transpose to swap the h and sequence length, since, we want all the sequence words to have access to embeddings\n",
    "        \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        #print(query.shape)\n",
    "        \n",
    "        ## Now we perform scaled dot product here\n",
    "        output, self.attention_weights = MultiHeadAttention.scaled_dot_product_attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        #concatination part happens here\n",
    "        #output's dimension is batch_size, h, sequence_length, d_k we combine\n",
    "        \n",
    "        #hen you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\n",
    "        output = output.transpose(1,2).contiguous().view(output.shape[0], -1 , self.d_k * self.h)\n",
    "        \n",
    "        #apply the linear part by multiplying with the Linear layer i.e self.W_O\n",
    "        \n",
    "        return self.W_O(output)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c6413e",
   "metadata": {},
   "source": [
    "## FeedForward Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac64685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, d_model, dff, dropout):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, dff)\n",
    "        self.layer2 = nn.Linear(dff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #x has dimension of (batch_size, sequence length, d_model)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c47da202",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkff = FeedForwardNeuralNetwork(512,2048, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfa490da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkff(torch.rand(2,3,512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907b23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e210da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = MultiHeadAttention(256, 8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8901c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(8, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b29f008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "025024aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = attention(q,q,q,torch.rand(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fc27b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39913da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c62c899d",
   "metadata": {},
   "source": [
    "## Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddde6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    # here sublayer can be either Multihead attention or feedforward neural network see the figure for more info\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        #This is different from the paper, we add pre-layer normalization here which means\n",
    "        # we first add normalize and then add the sublayer and then the dropout\n",
    "        return x + self.dropout(sublayer(self.norm(x) ) )\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c54ec5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescon = ResidualConnection(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39f3e2",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b920ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, multi_head_attention: MultiHeadAttention , feed_forward_neural_network: FeedForwardNeuralNetwork, dropout:float ):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = multi_head_attention\n",
    "        self.feed_forward_neural_network = feed_forward_neural_network\n",
    "        \n",
    "#         self.ResidualConnectionForAtt = ResidualConnection(dropout)\n",
    "#         self.ResidualConnectionForFF = ResidualConnection(dropout)\n",
    "        \n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2) ])\n",
    "        \n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        # x has dimension of batch_size, sequence_length, embedding_dimension i.e d_model\n",
    "        \n",
    "        #ResidualConnection takes in sublayer that could be either the MultiHeadAttention or FeedForwardNeuralNetwork\n",
    "        x = self.residual_connection[0](x, lambda x: self.multi_head_attention(x,x,x, src_mask) )\n",
    "        \n",
    "        x = self.residual_connection[1](x, self.feed_forward_neural_network )\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39a9825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,3,256)\n",
    "encoder = EncoderBlock(MultiHeadAttention(256,8, 0.1), FeedForwardNeuralNetwork(256, 1024, 0.1), 0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18a31229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1097,  0.8400,  0.7316,  ..., -0.1349,  0.7983, -0.7609],\n",
       "         [ 0.4773,  0.5382,  1.3971,  ..., -0.1193, -0.1069, -0.0993],\n",
       "         [-0.2387,  0.5455,  1.1077,  ...,  0.6359,  1.0645,  0.5700]],\n",
       "\n",
       "        [[ 0.4563,  0.6745,  0.8319,  ...,  0.2010, -0.3039,  0.5891],\n",
       "         [ 1.0434,  0.0524,  0.2420,  ..., -0.1676,  0.2204,  0.5424],\n",
       "         [ 0.5644,  0.6332,  0.6126,  ...,  0.2478,  0.4651,  0.7320]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(x,torch.rand(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae551f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0ac3a90",
   "metadata": {},
   "source": [
    "### Whole Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0eaacd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers : nn.ModuleList ):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "    \n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7767828",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "h = 8\n",
    "dropout = 0.1 \n",
    "dff = 2048\n",
    "N = 6\n",
    "\n",
    "encoder_layers = []\n",
    "for _ in range(N):\n",
    "    # We define multi head attention and feed forward neural network in each iteration, because it needs to initialize different parameters for their weights\n",
    "    multi_head_attention = MultiHeadAttention(d_model, h, dropout)\n",
    "    feed_forward_neural_network = FeedForwardNeuralNetwork(d_model, dff, dropout)\n",
    "    encoder_block = EncoderBlock(multi_head_attention , feed_forward_neural_network, dropout)\n",
    "\n",
    "    encoder_layers.append(encoder_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cff0c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(nn.ModuleList(encoder_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe93b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c454899e",
   "metadata": {},
   "source": [
    "### Sample working for ResidualConnectinForAtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68899e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(q,k,v): #Imagine this is MultiHeadAttention layer\n",
    "    print(q+k+v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa3d0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(x, sublayer):\n",
    "    x = x + 2\n",
    "    return sublayer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63590b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "func2(1,lambda x: func1(x,x,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab111224",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48dd7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, multi_head_attention: MultiHeadAttention, cross_attention: MultiHeadAttention, feed_forward_neural_network: FeedForwardNeuralNetwork, dropout):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = multi_head_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.feed_forward_neural_network = feed_forward_neural_network\n",
    "        \n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "        \n",
    "    def forward(self, x,encoder_output, src_mask, target_mask ):\n",
    "        #Since this is the decoder block so it'll add the target mask, mask sizes are (sequence_length, sequence_length)\n",
    "        x = self.residual_connection[0](x, lambda x: self.multi_head_attention(x,x,x, target_mask ))\n",
    "        x = self.residual_connection[1](x, lambda x: self.cross_attention(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connection[2](x, self.feed_forward_neural_network )\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca3f1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Testing Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a234099",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderBlock(MultiHeadAttention(256,8, 0.1),MultiHeadAttention(256,8, 0.1), FeedForwardNeuralNetwork(256, 1024, 0.1), 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7013286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4113,  0.5963,  0.1499,  ...,  1.3048,  0.4361,  0.2128],\n",
       "         [ 0.6393,  1.2134,  1.0995,  ...,  1.1252,  1.3375, -0.3363],\n",
       "         [ 1.4473,  0.3154,  0.8292,  ...,  0.4736,  0.7927, -0.1694],\n",
       "         ...,\n",
       "         [ 1.0062,  0.3345,  0.4965,  ...,  1.4983,  0.9826, -0.4600],\n",
       "         [ 1.4223,  0.3861,  0.4384,  ...,  0.4903,  0.8880,  0.0317],\n",
       "         [ 1.0972,  1.0064,  1.0321,  ...,  0.8769,  0.2669,  0.8189]],\n",
       "\n",
       "        [[ 0.7311,  0.4588,  0.3404,  ...,  0.5960,  0.8431, -0.1105],\n",
       "         [ 1.5669,  0.2920, -0.0659,  ...,  0.3923,  0.9875,  0.1748],\n",
       "         [ 1.1427,  0.3443,  0.7031,  ...,  0.0298,  1.4210, -0.2770],\n",
       "         ...,\n",
       "         [ 1.5945,  0.6806,  0.1740,  ...,  0.8928,  0.6424, -0.1790],\n",
       "         [ 0.8587,  0.3813, -0.2758,  ...,  0.3957,  0.7064, -0.8708],\n",
       "         [ 1.3922,  1.1939,  0.5063,  ...,  0.6236,  0.7639, -0.0289]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(torch.rand(2,10, 256),torch.rand(2,10, 256), torch.rand(10,10), torch.rand(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b62bbf",
   "metadata": {},
   "source": [
    "### Whole Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ae88581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    def forward(self,x, encoder_output, src_mask, target_mask ):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, target_mask  )\n",
    "        return self.norm(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b325a",
   "metadata": {},
   "source": [
    "## Linear Transformation\n",
    "Projecting each embedding to a word in vocabulory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1313196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTransformation(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        # This is predicting what comes after each word.\n",
    "        \n",
    "        return torch.log_softmax(self.linear(x), dim = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "af149136",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = LinearTransformation(256, 342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f051fc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 342])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin(torch.rand(2,3,256)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787980ac",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a59a5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, projection: LinearTransformation,source_embedding: InputEmbedding, target_embedding: InputEmbedding, source_positional_encoding: PositionalEncoding, target_positional_encoding: PositionalEncoding ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.projection = projection\n",
    "        self.source_embedding = source_embedding\n",
    "        self.target_embedding = target_embedding\n",
    "        self.source_positional_embedding = source_positional_encoding\n",
    "        self.target_positional_embedding = target_positional_encoding\n",
    "        \n",
    "    def encode(self,x):\n",
    "        # positional encoding function will concatinate the input embedding with the positional encodings. \n",
    "        x = self.source_embedding(x)\n",
    "        x = self.source_positional_embedding(x)\n",
    "        return self.encoder(x)\n",
    "        \n",
    "    def decode(self,x, encoder_output,src_mask, target_mask):\n",
    "        x = self.target_embedding(x)\n",
    "        x = self.target_positional_embedding(x)\n",
    "        \n",
    "        return self.decoder(x,encoder_output,src_mask, target_mask )\n",
    "        \n",
    "        \n",
    "    def project(self, x):\n",
    "        \n",
    "        return self.projection(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56306493",
   "metadata": {},
   "source": [
    "## Building the Transformers\n",
    "\n",
    "This is where we actually build the transformers. Now that we have implemented all the classes, we will go on to build the actual transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "856ae176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildingTransformer(source_vocab_size,target_vocab_size, source_seq_length:int, target_seq_length: int, d_model: int = 512, dropout: float = 0.1, dff: int = 2048 , h:int = 8, N:int = 6):\n",
    "    print('hello')\n",
    "    #Encoder, Decoder, LinearTransformation, InputEmbedding, PositionalEncoding\n",
    "    #MultiHeadAttention, FeedForwardNeuralNetwork, EncoderBlock, DecoderBlock\n",
    "    \n",
    "    source_embedding = InputEmbedding(source_vocab_size, d_model)\n",
    "    target_embedding = InputEmbedding(target_vocab_size, d_model)\n",
    "    \n",
    "    source_positional_encoding = PositionalEncoding(d_model, source_seq_length, dropout)\n",
    "    target_positional_encoding = PositionalEncoding(d_model, target_seq_length, dropout)\n",
    "    \n",
    "    multi_head_attention = MultiHeadAttention(d_model, h, dropout)\n",
    "    feed_forward_neural_network = FeedForwardNeuralNetwork(d_model, dff, dropout)\n",
    "    \n",
    "    \n",
    "    \n",
    "    encoder_layers = []\n",
    "    for _ in range(N):\n",
    "        # We define multi head attention and feed forward neural network in each iteration, because it needs to initialize different parameters for their weights\n",
    "        multi_head_attention = MultiHeadAttention(d_model, h, dropout)\n",
    "        feed_forward_neural_network = FeedForwardNeuralNetwork(d_model, dff, dropout)\n",
    "        encoder_block = EncoderBlock(multi_head_attention , feed_forward_neural_network, dropout)\n",
    "        \n",
    "        encoder_layers.append(encoder_block)\n",
    "        \n",
    "\n",
    "    \n",
    "    decoder_layers = []\n",
    "    for _ in range(N):\n",
    "            \n",
    "        multi_head_attention = MultiHeadAttention(d_model, h, dropout)\n",
    "        feed_forward_neural_network = FeedForwardNeuralNetwork(d_model, dff, dropout)\n",
    "        self_attention = MultiHeadAttention(d_model, h, dropout)\n",
    "        decoder_block = DecoderBlock(multi_head_attention ,self_attention, feed_forward_neural_network, dropout)\n",
    "        \n",
    "        decoder_layers.append(decoder_block)\n",
    "\n",
    "        \n",
    "    encoder = Encoder(nn.ModuleList(encoder_layers))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_layers))\n",
    "    \n",
    "    \n",
    "    linear_transformation = LinearTransformation(d_model, target_vocab_size)\n",
    "    \n",
    "    ##define the whole transformer\n",
    "    transformer = Transformer(encoder, decoder, linear_transformation, source_embedding, target_embedding, source_positional_encoding, target_positional_encoding )\n",
    "    \n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer\n",
    "            \n",
    "    \n",
    "    \n",
    "    #I guess this should do the job\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d367984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "tf = buildingTransformer(200,200, 5, 5, 512, 0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6778ef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mencoder(torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m512\u001b[39m), torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[60], line 9\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m----> 9\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, mask)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, x, src_mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# x has dimension of batch_size, sequence_length, embedding_dimension i.e d_model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#ResidualConnection takes in sublayer that could be either the MultiHeadAttention or FeedForwardNeuralNetwork\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connection[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_attention(x,x,x, src_mask) )\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connection[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_neural_network )\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m, in \u001b[0;36mResidualConnection.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sublayer):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#This is different from the paper, we add pre-layer normalization here which means\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# we first add normalize and then add the sublayer and then the dropout\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sublayer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x) ) )\n",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m, in \u001b[0;36mEncoderBlock.forward.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# x has dimension of batch_size, sequence_length, embedding_dimension i.e d_model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#ResidualConnection takes in sublayer that could be either the MultiHeadAttention or FeedForwardNeuralNetwork\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connection[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_attention(x,x,x, src_mask) )\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connection[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_neural_network )\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 61\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     56\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#print(query.shape)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m## Now we perform scaled dot product here\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_weights \u001b[38;5;241m=\u001b[39m MultiHeadAttention\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(query, key, value, mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#concatination part happens here\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#output's dimension is batch_size, h, sequence_length, d_k we combine\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#hen you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh)\n",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m, in \u001b[0;36mMultiHeadAttention.scaled_dot_product_attention\u001b[0;34m(query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#masking\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     attention_weights \u001b[38;5;241m=\u001b[39m attention_weights\u001b[38;5;241m.\u001b[39mmasked_fill_(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)\n\u001b[1;32m     31\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m attention_weights\u001b[38;5;241m.\u001b[39msoftmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#dropout\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "encoder_output = tf.encoder(torch.rand(2,5,512), torch.rand(2,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8571c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnings",
   "language": "python",
   "name": "learnings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
