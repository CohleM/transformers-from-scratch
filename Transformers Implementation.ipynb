{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f45122",
   "metadata": {},
   "source": [
    "## MultiHead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f048b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92137807",
   "metadata": {},
   "source": [
    "## Input Embeddings\n",
    "\n",
    "Since, models don't have notion of words, We convert every word in a sequence into a vector representation of specific dimension (256 or 512 ...). There are certain processes involved in doing so. \n",
    "\n",
    "First, we should have a vocabulary, For example: think of it as a dictionary which contains letter a,b,c,d...z and the letters have their own indexes such as a = 0, b = 1, ... z = 25. so when we have a new letter presented to us, this embedding layer will be applied to that letter. for example c,d will be mapped to 3,4. \n",
    "\n",
    "So instead of having letter in our vocabulary we have words. So, with the help of this vocabulary we map our words in the sequence to the index in its original vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a6cb4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## embeddings \n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a batch of sequence of words, batch_size, sequence_length -> batch_size, sequence_length, d_model\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a29c4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14b470",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "In Layer Normalization, normalization is done across all the features $x_{i,k}$  than across all the batches, this prcoess removes the dependency input sequences with each other.\n",
    "\n",
    "First, We calculate mean and standard deviation. \n",
    "\n",
    "\\begin{gather} \\mu_i = \\frac{1}{K} \\sum_{k=1}^{K} x_{i,k} \\\\ \\sigma_i^2 = \\frac{1}{K} \\sum_{k=1}^{K} (x_{i,k} - \\mu_i)^2 \\\\ \\end{gather}\n",
    "\n",
    "\n",
    "Then we normalize each sample such that the elements in the sample have zero mean and unit variance. \n",
    "Ïµ\n",
    " is for numerical stability in case the denominator becomes zero by chance.\n",
    " \n",
    " $$\\hat{x}_{i,k} = \\frac{x_{i,k}-\\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$$\n",
    " \n",
    " Finally, there is a scaling and shifting step. \n",
    "Î³\n",
    " and \n",
    "Î²\n",
    " are learnable parameters.\n",
    " \n",
    " $$y_i = \\gamma \\hat{x}_{i} + \\beta \\equiv {\\text{LN}}_{\\gamma, \\beta} (x_i)$$\n",
    " \n",
    "These parameters $\\gamma$ and $\\beta$ introduce fluctuations in the normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "42b3cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 10**-6\n",
    "        \n",
    "        ## specifying nn.Parameter will add requires_grad_ to that parameter so it will be a learnable parameter.\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.ones(1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # (batch, seq_len, 1)\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        # (batch, seq_len, 1)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "        \n",
    "        # dimension is batch,seq_len, d_model\n",
    "        x = ( self.gamma * (x - mean) / (std + self.eps) ) + self.beta\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2b7784a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LN = LayerNormalization(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d1dc9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LN(torch.rand(2,3,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "10b4ded8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 256])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "43219aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = InputEmbedding(2000,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3552f9b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myoo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[93], line 12\u001b[0m, in \u001b[0;36mInputEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# x is a batch of sequence of words, batch_size, sequence_length -> batch_size, sequence_length, d_model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "embed('yoo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1989d5",
   "metadata": {},
   "source": [
    "![(https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png)](https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c7c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65640045",
   "metadata": {},
   "source": [
    "## MultiHeadAttention\n",
    "\n",
    "Here comes the most important mechanism that powers whole the LLM industry, **attention** \n",
    "\n",
    "Attention in general refers to focus on some specific section of words while ignoring others. Attention is used to understand the context of a word in a sequence.\n",
    "\n",
    "The concept of self-attention is to utilize the entire sequence to compute a weighted average of each token's embedding instead of relying on a fixed embedding for each token like word2vec. Embeddings that are generated this way are called contextualized embeddings. This can be restated as self-attention generating a new sequence of embeddings $x_1', \\ldots, x_n'$ when given a sequence of token embeddings $x_1, \\ldots, x_n$, where each new embedding $x_i'$ is a linear combination of all the $x_j$ in the sequence.\n",
    "\n",
    "$x_i' = \\sum_{j=1}^{n} w_{ji} x_j$\n",
    "\n",
    "The coefficients $w_{ji}$ are called attention weights and are normalized so that:\n",
    "\n",
    "$\\sum_{j=1}^{n} w_{ji} = 1$\n",
    "\n",
    "So, the magic of paying attention is enabled by these attention weights.\n",
    "\n",
    "For example, letâ€™s consider these two sentences.\n",
    "\n",
    "    I love cool, crisp fall weather.\n",
    "    Donâ€™t fall on your way to the gym.\n",
    "\n",
    "  The word fall in the first sentence denotes weather by looking at words like cool and crips whereas fall in the second sentence denotes actually falling by looking at words like way and gym.\n",
    "\n",
    "\n",
    "Let's discuss how we construct attention weights and the final embedding representation.\n",
    "\n",
    "### Scaled dot-product attention\n",
    "\n",
    "Scaled Dot-Product Attention is used to calculate the attention weights.\n",
    "The first step in calculating self-attention is to project three vectors from each of the encoderâ€™s input vectors (in this case, token embeddings). So for each word, we project three matrices,  ð‘„,ð¾,ð‘‰  and which are called Query vector, a Key vector, and a Value vector and each has a dimension of  ð‘‘ð‘˜ . \n",
    "\n",
    "These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\n",
    "The dot product acts as a similarity function which determines how much the query and key vectors relate to each other. If queries and keys are similar, they will result in a significant dot product.\n",
    "\n",
    "The output is calculated as a weighted sum of the values, with each value's weight determined by the query's compatibility function with its corresponding key.\n",
    "\n",
    "\n",
    "\n",
    "To obtain the final weights on the values, first, the dot product of the query with all keys is computed and then normalized by $\\sqrt{d_{k}}$. Then a softmax function is applied. Finally, $V$ is multiplied with the previous output.\n",
    "\n",
    "The final output is:\n",
    "\n",
    "$Attention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "The intuition is that the softmax reweights between 0 and 1 (kind of like probability for each word), which sums upto 1 for all the words. So, multiplying this probability with $V$ determines the contribution of each word against each other. \n",
    "\n",
    "\n",
    "\n",
    "If you don't understand the how we calculate the attention weights and the final embedding representation then, let's understand this with the help of an analogy.\n",
    "\n",
    "Suppose you want to make something to eat for dinner. But you don't know how to. But you've got a Recepie book that tells you what ingredients to use let the Recepie book be our QUERY, now you go to a supermarket to buy these recepies, the ingredients the supermarket has in their shelves is the KEY, and now you look at your recepie book and ingredients in the shelves to find how similar they are, which becomes our attention weights. Now that you've found how similar these ingredients are, you update your shopping cart based on the similarity, which is, multiplying the attention weights with VALUES. That's it Now you have your ingredient, which is the embedding representation which will be used for Language modeling later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "18f60f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"This class resembles to the sequence of the above multiheadattenion picture\"\"\"\n",
    "    ##self, input_sequence, head_size, embedding_dimention\n",
    "    def __init__(self, h: int, d_model: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.h = h\n",
    "        \n",
    "        assert d_model % h == 0, \"d_model is not divisible by head\"\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias = False)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(query, key, value, mask = None, dropout = None):\n",
    "        d_k = query.shape[-1]\n",
    "        #dot product between Q and K\n",
    "        attention_weights = query @ key.transpose(-2,-1)\n",
    "        \n",
    "        #scaling\n",
    "        attention_weights = attention_weights / math.sqrt(d_k)\n",
    "        \n",
    "        #masking\n",
    "        if mask is not None:\n",
    "            attention_weights = attention_weights.masked_fill_(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = attention_weights.softmax(dim = -1)\n",
    "        \n",
    "        #dropout\n",
    "        if dropout is not None:\n",
    "            attention_weights = nn.Dropout(dropout)\n",
    "            \n",
    "        return attention_weights @ key, attention_weights\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, q, k, v, mask = None, dropout = None):\n",
    "        \n",
    "        #q,k,v are embeddings of whole batch of sequence, so their size would be batch_size, sequence_length, d_model (embedding dimension)\n",
    "        query = self.W_Q(q)\n",
    "        key = self.W_K(k)\n",
    "        value = self.W_V(v)\n",
    "        \n",
    "        #divide the q,k,v into different h heads\n",
    "        \n",
    "        #query initially had size of (batch_size, sequence_length, d_model)\n",
    "        #and we split the d_model which is the embedding into different heads with each size of d_k = d_model/h\n",
    "        #We finally call transpose to swap the h and sequence length, since, we want all the sequence words to have access to embeddings\n",
    "        \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        #print(query.shape)\n",
    "        \n",
    "        ## Now we perform scaled dot product here\n",
    "        output, self.attention_weights = MultiHeadAttention.scaled_dot_product_attention(query, key, value)\n",
    "        \n",
    "        #concatination part happens here\n",
    "        #output's dimension is batch_size, h, sequence_length, d_k we combine\n",
    "        \n",
    "        #hen you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\n",
    "        output = output.transpose(1,2).contiguous().view(output.shape[0], -1 , self.d_k * self.h)\n",
    "        \n",
    "        #apply the linear part by multiplying with the Linear layer i.e self.W_O\n",
    "        \n",
    "        return self.W_O(output), self.attention_weights\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456ce32",
   "metadata": {},
   "source": [
    "## FeedForward Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3d363627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, d_model, dff, dropout):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, dff)\n",
    "        self.layer2 = nn.Linear(dff, d_model)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #x has dimension of (batch_size, sequence length, d_model)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0e8da653",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkff = FeedForwardNeuralNetwork(512,2048, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0f8309e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkff(torch.rand(2,3,512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa9a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6e9953a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = MultiHeadAttention(8, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "242bfba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(8, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c8f84b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 256])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "92bfd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = attention.forward(q,q,q, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6db60334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 256])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "82538ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 10, 10])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358bd25e",
   "metadata": {},
   "source": [
    "## Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "df5b09eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    # here sublayer can be either Multihead attention or feedforward neural network see the figure for more info\n",
    "    def __init__(self, sublayer, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization()\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #This is different from the paper, we add pre-layer normalization here which means\n",
    "        # we first add normalize and then add the sublayer and then the dropout\n",
    "        return x + self.dropout(sublayer(self.norm(x) ) )\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e9c8bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescon = ResidualConnection( attention, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f070a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnings",
   "language": "python",
   "name": "learnings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
