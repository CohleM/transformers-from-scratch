{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e47767",
   "metadata": {},
   "source": [
    "## MultiHead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e3a2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1679c1c",
   "metadata": {},
   "source": [
    "![(https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png)](https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7dad0f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## embeddings \n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a batch of sequence of words, batch_size, sequence_length -> batch_size, sequence_length, d_model\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4fab9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bbf253",
   "metadata": {},
   "source": [
    "In Layer Normalization, normalization is done across all the features $x_{i,k}$  than across all the batches, this prcoess removes the dependency input sequences with each other.\n",
    "\n",
    "First, We calculate mean and standard deviation. \n",
    "\n",
    "\\begin{gather} \\mu_i = \\frac{1}{K} \\sum_{k=1}^{K} x_{i,k} \\\\ \\sigma_i^2 = \\frac{1}{K} \\sum_{k=1}^{K} (x_{i,k} - \\mu_i)^2 \\\\ \\end{gather}\n",
    "\n",
    "\n",
    "Then we normalize each sample such that the elements in the sample have zero mean and unit variance. \n",
    "ϵ\n",
    " is for numerical stability in case the denominator becomes zero by chance.\n",
    " \n",
    " $$\\hat{x}_{i,k} = \\frac{x_{i,k}-\\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$$\n",
    " \n",
    " Finally, there is a scaling and shifting step. \n",
    "γ\n",
    " and \n",
    "β\n",
    " are learnable parameters.\n",
    " \n",
    " $$y_i = \\gamma \\hat{x}_{i} + \\beta \\equiv {\\text{LN}}_{\\gamma, \\beta} (x_i)$$\n",
    " \n",
    "These parameters $\\gamma$ and $\\beta$ introduce fluctuations in the normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3e07f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = InputEmbedding(2000,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4b7a12a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myoo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[93], line 12\u001b[0m, in \u001b[0;36mInputEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# x is a batch of sequence of words, batch_size, sequence_length -> batch_size, sequence_length, d_model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnings/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "embed('yoo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c8ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "18f60f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"This class resembles to the sequence of the above multiheadattenion picture\"\"\"\n",
    "    ##self, input_sequence, head_size, embedding_dimention\n",
    "    def __init__(self, h: int, d_model: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.h = h\n",
    "        \n",
    "        assert d_model % h == 0, \"d_model is not divisible by head\"\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias = False)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(query, key, value, mask = None, dropout = None):\n",
    "        d_k = query.shape[-1]\n",
    "        #dot product between Q and K\n",
    "        attention_weights = query @ key.transpose(-2,-1)\n",
    "        #scaling\n",
    "        \n",
    "        attention_weights = attention_weights / math.sqrt(d_k)\n",
    "        \n",
    "        #masking\n",
    "        if mask is not None:\n",
    "            attention_weights = attention_weights.masked_fill_(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = attention_weights.softmax(dim = -1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attention_weights = nn.Dropout(dropout)\n",
    "            \n",
    "        return attention_weights @ key, attention_weights\n",
    "        #dropout\n",
    "        \n",
    "    \n",
    "    def forward(self, q, k, v, mask = None, dropout = None):\n",
    "        \n",
    "        #q,k,v are embeddings of whole batch of sequence, so their size would be batch_size, sequence_length, d_model (embedding dimension)\n",
    "        query = self.W_Q(q)\n",
    "        key = self.W_K(k)\n",
    "        value = self.W_V(v)\n",
    "        \n",
    "        #divide the q,k,v into different h heads\n",
    "        \n",
    "        #query initially had size of (batch_size, sequence_length, d_model)\n",
    "        #and we split the d_model which is the embedding into different heads with each size of d_k = d_model/h\n",
    "        #We finally call transpose to swap the h and sequence length, since, we want all the sequence words to have access to embeddings\n",
    "        \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        #print(query.shape)\n",
    "        \n",
    "        ## Now we perform scaled dot product here\n",
    "        output, self.attention_weights = MultiHeadAttention.scaled_dot_product_attention(query, key, value)\n",
    "        \n",
    "        #concatination part happens here\n",
    "        #output's dimension is batch_size, h, sequence_length, d_k we combine\n",
    "        \n",
    "        #hen you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\n",
    "        output = output.transpose(1,2).contiguous().view(output.shape[0], -1 , self.d_k * self.h)\n",
    "        \n",
    "        #apply the linear part by multiplying with the Linear layer i.e self.W_O\n",
    "        \n",
    "        return self.W_O(output), self.attention_weights\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0682f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d0571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee40eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431e352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5212d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = MultiHeadAttention(8, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "793cc1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(8, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7fac49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 256])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aefff248",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = attention.forward(q,q,q, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a6503875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 256])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "07b1f2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 10, 10])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8b895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnings",
   "language": "python",
   "name": "learnings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
